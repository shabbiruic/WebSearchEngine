{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from utilities import load_dict_from_file,load_dataFrame,save_dataFrame\n",
    "from Configuration import path_data_frame,path_to_vectorizer,path_to_document_tfidf,path_to_document_folder\n",
    "from string import digits\n",
    "\n",
    "\n",
    "\n",
    "# this removes the punctuation and gets the token.\n",
    "def removePunctuationAndGetTokens(lines):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    lines = lines.translate(remove_digits)\n",
    "#     pattern = re.compile('[0-9].*')\n",
    "#     lines = re.sub(pattern,' ',lines)\n",
    "#     print(lines)\n",
    "    translator = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "    lines = lines.translate(translator)\n",
    "    tokens = lines.split()\n",
    "    return tokens\n",
    "\n",
    "# takes list of words as a input and do stemming stopword removal and also removes token which are of length less than \n",
    "# or equal to two \n",
    "def removeSingleDoubleCharacterWordStopWordsAndStemming(tokens):\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [ps.stem(token) for token in tokens if token not in stop_words] \n",
    "#     print(tokens) \n",
    "    return ' '.join([token for token in tokens if len(token)>2 and token not in stop_words])\n",
    "\n",
    "\n",
    "# It creates the dataframe out of files which store URL and its content.\n",
    "def create_data_frame_from_documents(folderName):\n",
    "    df = pd.DataFrame()\n",
    "    for fileName in os.listdir(folderName):\n",
    "#         if fileName in ['10.txt','100.txt','103.txt','104.txt']:\n",
    "            pathName = folderName+'\\\\'+ fileName\n",
    "            fp = open(pathName, 'r',encoding='utf-8')\n",
    "            url = fp.readline().split(':',1)[1].replace('\\n','')\n",
    "            url = url.strip()\n",
    "            lines = fp.read()\n",
    "            content = removeSingleDoubleCharacterWordStopWordsAndStemming(removePunctuationAndGetTokens(lines))\n",
    "            doc_id = fileName.replace(\".txt\", \"\")\n",
    "            df = df.append({'Doc_id':int(doc_id),'URL':url,'Content':content},ignore_index=True)\n",
    "    return df\n",
    "        \n",
    "    \n",
    "# This calculate the tf-idf of all the URL contents    \n",
    "def calculate_tfidf_of_documents(dataframe_file):\n",
    "    \n",
    "    df = load_dataFrame(dataframe_file)\n",
    "    df.sort_values(by=['Doc_id'], inplace=True)\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfidf_documents=tfidf_vectorizer.fit_transform(df['Content'].values.astype('U'))\n",
    "    return tfidf_vectorizer,tfidf_documents\n",
    "\n",
    "\n",
    "def load_vectors(filename):\n",
    "    with open(filename+'.pk', 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "\n",
    "def save_vectors(vector,filename):\n",
    "    pickle.dump(vector, open(filename+'.pk', 'wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# This function calculates the TF-IDF of query depending on the vectorizer we have saved earlier\n",
    "def calculate_tfidf_of_query(query,tfidf_vectorizer):\n",
    "    processed_query = removeSingleDoubleCharacterWordStopWordsAndStemming(removePunctuationAndGetTokens(query))\n",
    "    return tfidf_vectorizer.transform([query])\n",
    "    \n",
    "# Main function that triggers the complete TF-IDF calculation process\n",
    "def main():\n",
    "    docu_df = create_data_frame_from_documents(path_to_document_folder)\n",
    "    save_dataFrame(docu_df,path_data_frame)\n",
    "    vectorizer, doc_tf_idf = calculate_tfidf_of_documents(path_data_frame)\n",
    "    save_vectors(vectorizer,path_to_vectorizer)\n",
    "    save_vectors(doc_tf_idf,path_to_document_tfidf)\n",
    "    \n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
